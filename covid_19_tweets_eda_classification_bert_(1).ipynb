{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"covid_19_tweets_eda_classification_bert_(1).ipynb","provenance":[],"collapsed_sections":["P7zXj91yLpxc","C12qgqlmLpyE","l65zJH6LLpyE","CdLPZtecLpyh","s30bQhY_Lpyl"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"UWvooHkZLpvE"},"source":["# 1. Introduction<a name=\"introduction\"></a>\n","### With this notebook, we will attempt to find the best model to classify these tweets. We'll start with some data cleaning and some vizualizations to get a little more familiar with the data, and from there we'll explore some more traditional classification methods.  After that, we'll train BERT on the tweets and compare the results with other models. "]},{"cell_type":"code","metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"3pp4cKiCLpvN","executionInfo":{"status":"ok","timestamp":1625760262185,"user_tz":-330,"elapsed":4367,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}},"outputId":"d10d0f78-b1d2-4b18-96f9-32dd0147b3dc"},"source":["!pip install chart_studio"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting chart_studio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/ce/330794a6b6ca4b9182c38fc69dd2a9cbff60fd49421cb8648ee5fee352dc/chart_studio-1.1.0-py3-none-any.whl (64kB)\n","\r\u001b[K     |█████                           | 10kB 24.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 28.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 19.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.15.0)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from chart_studio) (4.4.1)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from chart_studio) (1.3.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from chart_studio) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->chart_studio) (2.10)\n","Installing collected packages: chart-studio\n","Successfully installed chart-studio-1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w1PmDg0caQZU","executionInfo":{"status":"ok","timestamp":1625760263628,"user_tz":-330,"elapsed":1447,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["import numpy as np \n","import pandas as pd \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re \n","import string\n","import requests\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.pipeline import Pipeline\n","from sklearn.ensemble import RandomForestClassifier\n","from wordcloud import WordCloud\n","import chart_studio.plotly as py\n","import plotly.graph_objects as go\n","from plotly.offline import download_plotlyjs, plot, iplot\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import TweetTokenizer\n","\n","from PIL import Image\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","import os\n","from os import path"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"FenhG8YTaQZd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625760263629,"user_tz":-330,"elapsed":27,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}},"outputId":"f13cf524-3f32-4b04-853e-3d049ebbd2b0"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Awl5ToXsaQZo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625760264025,"user_tz":-330,"elapsed":419,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}},"outputId":"ef33098c-c34b-4271-ec3f-2603141ff652"},"source":["import nltk\n","nltk.download('wordnet')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"jv-Bvu1XaQZu","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1625760265311,"user_tz":-330,"elapsed":1289,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}},"outputId":"1aea0d7e-6cc7-42b2-d43f-5c5b15c85102"},"source":["# Load in training data\n","train = pd.read_csv('Corona_NLP_train.csv', encoding = 'latin1')\n","# Copy training data\n","df = train.copy()\n","df.head()"],"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a4012d52912b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load in training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Corona_NLP_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Copy training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1992\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1993\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1994\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Corona_NLP_train.csv'"]}]},{"cell_type":"code","metadata":{"id":"wTbPtILGaQZ1","executionInfo":{"status":"aborted","timestamp":1625760265272,"user_tz":-330,"elapsed":30,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Load in test data\n","test_df = pd.read_csv('Corona_NLP_test.csv', encoding = 'latin1')\n","test_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZvL6hzdaQZ7"},"source":["# 2. Data Cleaning <a name=\"data_cleaning\"></a>"]},{"cell_type":"code","metadata":{"id":"wSKBPJu1aQZ8","executionInfo":{"status":"aborted","timestamp":1625760265273,"user_tz":-330,"elapsed":30,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Check for nulls\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0Swnf6CaQaB","executionInfo":{"status":"aborted","timestamp":1625760265274,"user_tz":-330,"elapsed":31,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Replace na with 'None'\n","df['Location'].fillna('None', inplace = True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ee6gxsE-aQaG","executionInfo":{"status":"aborted","timestamp":1625760265275,"user_tz":-330,"elapsed":32,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Join stopwords together and set them for use in cleaning function.\n","\", \".join(stopwords.words('english'))\n","stops = set(stopwords.words('english'))\n","\n","# Function that cleans tweets for classification. \n","def clean_tweet(tweet):\n","    # Remove hyperlinks.\n","    tweet= re.sub(r'https?://\\S+|www\\.\\S+','',tweet)\n","    # Remove html\n","    tweet = re.sub(r'<.*?>','',tweet)\n","    # Remove numbers (Do we want to remove numbers? Death toll?)\n","    tweet = re.sub(r'\\d+','',tweet)\n","    # Remove mentions\n","    tweet = re.sub(r'@\\w+','',tweet)\n","    # Remove punctuation\n","    tweet = re.sub(r'[^\\w\\s\\d]','',tweet)\n","    # Remove whitespace\n","    tweet = re.sub(r'\\s+',' ',tweet).strip()\n","    # Remove stopwords\n","    tweet = \" \".join([word for word in str(tweet).split() if word not in stops])\n","    \n","    return tweet.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAoloGXKaQaK","executionInfo":{"status":"aborted","timestamp":1625760265276,"user_tz":-330,"elapsed":33,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Check function\n","example2 = df['OriginalTweet'][1]\n","clean_tweet(example2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2dAhrlMaQaO","executionInfo":{"status":"aborted","timestamp":1625760265277,"user_tz":-330,"elapsed":34,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Apply text cleaning function to training and test dataframes.\n","df['newTweet'] = df['OriginalTweet'].apply(lambda x: clean_tweet(x))\n","test_df['newTweet'] = test_df['OriginalTweet'].apply(lambda x: clean_tweet(x))\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0a76VRAvLpwH"},"source":["### Here, we'll define a couple functions to either stem or lemmatize the tweets. These methods will be compared during classification to see which one gives us the model with the greatest accuracy. "]},{"cell_type":"code","metadata":{"id":"8R548iBgaQaU","executionInfo":{"status":"aborted","timestamp":1625760265278,"user_tz":-330,"elapsed":35,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["def token_stem(tweet):\n","    tk = TweetTokenizer()\n","    stemmer = PorterStemmer()\n","    tweet = tk.tokenize(tweet)\n","    tweet = [stemmer.stem(word) for word in tweet]\n","    tweet =  tweet = \" \".join([word for word in tweet])\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WsA1achaQaY","executionInfo":{"status":"aborted","timestamp":1625760265279,"user_tz":-330,"elapsed":35,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["def token_lemma(tweet):\n","    tk = TweetTokenizer()\n","    lemma = WordNetLemmatizer()\n","    tweet = tk.tokenize(tweet)\n","    tweet = [lemma.lemmatize(word) for word in tweet]\n","    tweet = \" \".join([word for word in tweet])\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61Byipz6aQad","executionInfo":{"status":"aborted","timestamp":1625760265279,"user_tz":-330,"elapsed":35,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["tweet = df['newTweet'][1]\n","tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGow3_nraQai","executionInfo":{"status":"aborted","timestamp":1625760265280,"user_tz":-330,"elapsed":36,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["print(token_stem(tweet))\n","print('\\n')\n","print(token_lemma(tweet))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vG3GT_sLpw8"},"source":["### See the differences in these techniques? Stemming converts words to their 'stems', while lemmatizing brings the words to their 'lemmas', or dictionary forms. "]},{"cell_type":"code","metadata":{"id":"qouCXKncaQam","executionInfo":{"status":"aborted","timestamp":1625760265282,"user_tz":-330,"elapsed":38,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["df['stemTweet'] = df['newTweet'].apply(lambda x: token_stem(x))\n","df['lemmaTweet'] = df['newTweet'].apply(lambda x: token_lemma(x))\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axLuYS8QaQar","executionInfo":{"status":"aborted","timestamp":1625760265283,"user_tz":-330,"elapsed":39,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Create more useful labels for classification.\n","# We will take the original 5 possibilites and\n","# reduce them to 3, removing the \"extremelys\".\n","def make_label(sentiment):\n","    \n","    label = ''\n","    if 'Positive' in sentiment: \n","        label = 1\n","    if 'Negative' in sentiment:\n","        label = -1\n","    if 'Neutral' in sentiment:\n","        label = 0\n","    return label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9hCBNCpaQau","executionInfo":{"status":"aborted","timestamp":1625760265285,"user_tz":-330,"elapsed":41,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Apply make_label funtion to training and test dataframes.\n","df['label'] = df['Sentiment'].apply(lambda x: make_label(x))\n","test_df['label'] = test_df['Sentiment'].apply(lambda x: make_label(x))\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amNvCoQyLpw_"},"source":["### Below are some of the common locations found in the tweets that will help us properly map more tweets to a particular country."]},{"cell_type":"code","metadata":{"id":"_wyPR2NYaQay","executionInfo":{"status":"aborted","timestamp":1625760265286,"user_tz":-330,"elapsed":41,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Some frequent US locations\n","us_filters = ('New York', 'New York, NY', 'NYC', 'NY', 'Washington, DC', 'Los Angeles, CA',\n","             'Seattle, Washington', 'Chicago', 'Chicago, IL', 'California, USA', 'Atlanta, GA',\n","             'San Francisco, CA', 'Boston, MA', 'New York, USA', 'Texas, USA', 'Austin, TX',\n","              'Houston, TX', 'New York City', 'Philadelphia, PA', 'Florida, USA', 'Seattle, WA',\n","             'Washington, D.C.', 'San Diego, CA', 'Las Vegas, NV', 'Dallas, TX', 'Denver, CO',\n","             'New Jersey, USA', 'Brooklyn, NY', 'California', 'Michigan, USA', 'Minneapolis, MN',\n","             'Virginia, USA', 'Miami, FL', 'Texas', 'Los Angeles', 'United States', 'San Francisco',\n","             'Indianapolis, IN', 'Pennsylvania, USA', 'Phoenix, AZ', 'New Jersey', 'Baltimore, MD',\n","             'CA', 'FL', 'DC', 'TX', 'IL', 'MA', 'PA', 'GA', 'NC', 'NJ', 'WA', 'VA', 'PAK', 'MI', 'OH',\n","             'CO', 'AZ', 'D.C.', 'WI', 'MD', 'MO', 'TN', 'Florida', 'IN', 'NV', 'MN', 'OR','LA', 'Michigan',\n","             'CT', 'SC', 'OK', 'Illinois', 'Ohio', 'UT', 'KY', 'Arizona', 'Colorado')\n","\n","# Various nation's frequent locations\n","uk_filters = ('England', 'London', 'london', 'United Kingdom', 'united kingdom',\n","              'England, United Kingdom', 'London, UK', 'London, England',\n","              'Manchester, England', 'Scotland, UK', 'Scotland', 'Scotland, United Kingdom',\n","              'Birmingham, England', 'UK', 'Wales')\n","india_filters = ('New Delhi, India', 'Mumbai', 'Mumbai, India', 'New Delhi', 'India', \n","                 'Bengaluru, India')\n","australia_filters = ('Sydney, Australia', 'New South Wales', 'Melbourne, Australia', 'Sydney',\n","                     'Sydney, New South Wales', 'Melbourne, Victoria', 'Melbourne', 'Australia')\n","canada_filters = ('Toronto, Ontario', 'Toronto', 'Ontario, Canada', 'Toronto, Canada', 'Canada',\n","                  'Vancouver, British Columbia', 'Ontario', 'Victoria', 'British Columbia', 'Alberta',)\n","south_africa_filters = ('Johannesburg, South Africa', 'Cape Town, South Africa', 'South Africa')\n","nigeria_filters = ('Lagos, Nigeria')\n","kenya_filters = ('Nairobi, Kenya')\n","france_filters = ('Paris, France')\n","ireland_filters = ('Ireland')\n","new_zealand_filters = ('New Zealand')\n","pakistan_filters = ('Pakistan')\n","malaysia_filters = ('Malaysia')\n","uganda_filters = ('Kampala, Uganda', 'Uganda')\n","singapore_filters = ('Singapore')\n","germany_filters = ('Germany', 'Deutschland')\n","switz_filters = ('Switzerland')\n","uae_filters = ('United Arab Emirates', 'Dubai')\n","spain_filters = ('Spain')\n","belg_filters = ('Belgium')\n","phil_filters = ('Philippines')\n","hk_filters = ('Hong Kong')\n","ghana_filters = ('Ghana')\n","# These all have large counts. Need to be removed from rest of data\n","other_filters = ('None', 'Worldwide', 'Global', 'Earth', '??')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ulJ3GFraQa1","executionInfo":{"status":"aborted","timestamp":1625760265287,"user_tz":-330,"elapsed":42,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["df['country'] = df['Location'].apply(lambda x: x.split(\",\")[-1].strip() if (\",\" in x) else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-d0d338aQa5","executionInfo":{"status":"aborted","timestamp":1625760265288,"user_tz":-330,"elapsed":43,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7LDuoTEaQa8","executionInfo":{"status":"aborted","timestamp":1625760265288,"user_tz":-330,"elapsed":43,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Changing strings found with filters into 3 digit codes\n","df['country'] = df['country'].apply(lambda x: 'USA' if x in us_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'GBR' if x in uk_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'IND' if x in india_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'AUS' if x in australia_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'CAN' if x in canada_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'ZAF' if x in south_africa_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'KEN' if x in kenya_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'NGA' if x in nigeria_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'SGP' if x in singapore_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'FRA' if x in france_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'NZL' if x in new_zealand_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'PAK' if x in pakistan_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'MYS' if x in malaysia_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'IRL' if x in ireland_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'UGA' if x in uganda_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'DEU' if x in germany_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'CHE' if x in switz_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'ARE' if x in uae_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'ESP' if x in spain_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'BEL' if x in belg_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'PHL' if x in phil_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'GHA' if x in ghana_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'HKG' if x in hk_filters else x)\n","df['country'] = df['country'].apply(lambda x: 'None' if x in other_filters else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPxmj94xaQbC","executionInfo":{"status":"aborted","timestamp":1625760265288,"user_tz":-330,"elapsed":43,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["df['country'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYOz09LILpxE"},"source":["# 3. Vizualizations <a name=\"viz\"></a>"]},{"cell_type":"code","metadata":{"id":"AjMdLWmqaQbF","executionInfo":{"status":"aborted","timestamp":1625760265289,"user_tz":-330,"elapsed":44,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# 0:30 because that's where the labeled countries end\n","places_df = pd.DataFrame(df['country'].value_counts()[0:30])\n","places_df.reset_index(inplace = True)\n","places_df.rename(columns = {'index':'Country', 'country':'Tweets'}, inplace = True)\n","# Remove 'None' location\n","places_df = places_df[places_df['Country'] != 'None']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxtHZ7cGaQbJ","executionInfo":{"status":"aborted","timestamp":1625760265289,"user_tz":-330,"elapsed":44,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["data = dict(type='choropleth',\n","            colorscale = 'inferno',\n","            locations = places_df['Country'],\n","            z = places_df['Tweets'],\n","            #locationmode = 'USA-states',\n","            text = places_df['Tweets'],\n","            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),\n","            colorbar = {'title':\"Number of Tweets\"}\n","            ) \n","\n","layout = dict(title = 'Number of Tweets By Country',\n","              geo = dict(#scope='usa',\n","                         showlakes = False,\n","                         lakecolor = 'rgb(85,173,240)',\n","                         projection_type='equirectangular')\n","             )\n","\n","choromap = go.Figure(data = [data],layout = layout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0zOaY7DhaQbL","executionInfo":{"status":"aborted","timestamp":1625760265290,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["iplot(choromap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6bVxMszLpxH"},"source":["### The vast majority of tweets come from English speaking countries, which makes sense, since these tweets are all in English. Largest contributer is the USA followed by the UK and Canada.  \n"]},{"cell_type":"code","metadata":{"id":"e9ipqnZsaQbO","executionInfo":{"status":"aborted","timestamp":1625760265290,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["#image courtesy of https://tse2.mm.bing.net/th?id=OIP.VLv_PpEOc8TDwuTNvj5hWQHaHa&pid=Api\n","img = Image.open('rona4.jpeg')\n","mask = np.array(Image.open('rona4.jpeg'))\n","\n","# Positive WordCloud\n","pos_df = df[df['label'] == 1]\n","pos_text = pos_df['newTweet'].to_string(index = False)\n","pos_text = re.sub(r'\\n','',pos_text)\n","pos_cloud = WordCloud(colormap = 'Greens', mask = mask).generate(pos_text)\n","\n","# Neutral WordCloud\n","neut_df = df[df['label'] == 0]\n","neut_text = neut_df['newTweet'].to_string(index = False)\n","neut_text = re.sub(r'\\n','', neut_text)\n","neut_cloud = WordCloud(colormap = 'Blues', mask = mask).generate(neut_text)\n","\n","# Negative wordcloud\n","neg_df = df[df['label'] == -1]\n","neg_text = neg_df['newTweet'].to_string(index = False)\n","neg_text = re.sub(r'\\n','', neg_text)\n","neg_cloud = WordCloud(colormap = 'Reds', mask = mask).generate(neg_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SNoTZKBaQbS","executionInfo":{"status":"aborted","timestamp":1625760265290,"user_tz":-330,"elapsed":44,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = [30,20])\n","ax1.imshow(pos_cloud)\n","ax1.set_title('Positive Cloud', fontsize = 30)\n","ax1.axis('off')\n","ax2.imshow(neut_cloud)\n","ax2.set_title('Neutral Cloud', fontsize = 30)\n","ax2.axis('off')\n","ax3.imshow(neg_cloud)\n","ax3.set_title('Negative Cloud', fontsize = 30)\n","ax3.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOM1mAUnaQbW"},"source":["### Tried to use an image of the coronavirus for the mask, it certainly could have turned out better...\n","### 'Grocery store', 'price', 'supermarket', and 'online shopping' being frequent in positive, neutral, and negative tweets is interesting.  Some stand-out negative terms are 'panic buying' and 'toilet paper'. For positive, 'hand sanitizer' catches my attention. "]},{"cell_type":"code","metadata":{"id":"Ca2164oKaQbW","executionInfo":{"status":"aborted","timestamp":1625760265291,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["def ngram_df(corpus,nrange,n=None):\n","    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n","    bag_of_words = vec.transform(corpus)\n","    sum_words = bag_of_words.sum(axis=0) \n","    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n","    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n","    total_list=words_freq[:n]\n","    df=pd.DataFrame(total_list,columns=['text','count'])\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gs3mSg2PaQbZ","executionInfo":{"status":"aborted","timestamp":1625760265291,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["unigram_df = ngram_df(df['newTweet'],(1,1),20)\n","bigram_df = ngram_df(df['newTweet'],(2,2),20)\n","trigram_df = ngram_df(df['newTweet'],(3,3),20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmzloFDqaQbb","executionInfo":{"status":"aborted","timestamp":1625760265292,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["unigram_df['text'][::-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGPn83oUaQbf","executionInfo":{"status":"aborted","timestamp":1625760265292,"user_tz":-330,"elapsed":44,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["sns.set(font_scale = 1.3)\n","sns.set(rc={'figure.figsize':(11.7,8.27)})\n","sns.barplot(data = unigram_df, y = 'text', x = 'count')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Mpr-E1LLpxa"},"source":["### 'Prices' being the most frequent unigram after covid/coronavirus may be due to rising food prices and other various shortages."]},{"cell_type":"code","metadata":{"id":"NUrL8oCdaQbi","executionInfo":{"status":"aborted","timestamp":1625760265293,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["sns.set(font_scale = 1.3)\n","sns.set(rc={'figure.figsize':(11.7,8.27)})\n","sns.barplot(data = bigram_df, y = 'text', x = 'count')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mF3EDohLLpxb"},"source":["### Grocery store way outpacing covid bigrams is pretty interesting. Online shopping, hand sanitizer, toilet paper, and panic buying are all within the realm of expectation. "]},{"cell_type":"code","metadata":{"id":"iOQ_GzrNaQbl","executionInfo":{"status":"aborted","timestamp":1625760265293,"user_tz":-330,"elapsed":45,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["sns.set(font_scale = 1.3)\n","sns.set(rc={'figure.figsize':(11.7,8.27)})\n","sns.barplot(data = trigram_df, y = 'text', x = 'count')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P7zXj91yLpxc"},"source":["### Grocery store dominates these trigrams. People may be concerned about the safety of grocery shopping during a pandemic, and the health of the grocery store workers. "]},{"cell_type":"markdown","metadata":{"id":"2b0WAl_naQbo"},"source":["# 4. Classification <a name=\"classification\"></a>"]},{"cell_type":"code","metadata":{"id":"Sw_FN142aQbs","executionInfo":{"status":"aborted","timestamp":1625760265294,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Set X and y.\n","X = df['newTweet']\n","y = df['label']\n","\n","# Split data into training and test sets.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmYeksfTLpxr","executionInfo":{"status":"aborted","timestamp":1625760265294,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["X[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vq5zX0VLLpxs"},"source":["### We'll try 4 different classifiers here: SVC, Logisitic Regression, Naive Bayes, and Random Forest. Furthermore, we'll also be testing whether these models perform better using Term Frequency Inverse Document Frequency or just a simple Count for the vectors we feed into the model. TFIDF increases every time a word appears in a document(tweet), but is then offset for every document(tweet) that word appears. This can help pick out the more important words for classification. Additionally, we'll be using cross validation to help gauge each model's accuracy and variance across multiple splits of the data. "]},{"cell_type":"code","metadata":{"id":"nue-W28paQbv","executionInfo":{"status":"aborted","timestamp":1625760265295,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["clf = dict({'SVC': LinearSVC(max_iter = 5000),\n","            'Logisitc': LogisticRegression(max_iter = 5000),\n","            'NaiveBayes': MultinomialNB(),\n","            'RandomForest': RandomForestClassifier(),\n","           })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOkZbBWdaQby","executionInfo":{"status":"aborted","timestamp":1625760265295,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["def make_models(clf, vectorizer, X_train, y_train, cv = 5):\n","    \n","    acc_df = pd.DataFrame(index=range(cv * len(clf)))\n","    results = []\n","    for classifier in clf.keys():\n","        model = Pipeline([('vectorizer',vectorizer),\n","                   ('clf', clf[classifier])])\n","        model.fit(X_train, y_train)\n","        scores = cross_val_score(model, X_train , y_train, cv = cv)\n","        model_name = classifier\n","        for fold, score in enumerate(scores):\n","            results.append((model_name, fold, score))\n","    \n","    acc_df = pd.DataFrame(results, columns=['model_name', 'fold', 'accuracy'])\n","    \n","    return acc_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KW-812DpaQb0","executionInfo":{"status":"aborted","timestamp":1625760265296,"user_tz":-330,"elapsed":47,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Number of folds for K-fold cross validation\n","cv = 10"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a4z7o9_jLpxu"},"source":["### Takes a good bit to run (over 30 minutes)...10 fold cross validation on 4 separate classifiers will take a while.\n","### Results are saved to 'pipe_results.csv' if you want to save time. \n","### Skip down a few cells to see where I load the results if you don't want to run each model.\n","### Logistic and RandomForest take much longer than SVC and NaiveBayes."]},{"cell_type":"code","metadata":{"id":"8ERU6YvdLpxw","executionInfo":{"status":"aborted","timestamp":1625760265296,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Skip to here to avoid running the models\n","all_df = pd.read_csv('pipe_results.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WhDsw2neaQcL","executionInfo":{"status":"aborted","timestamp":1625760265297,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["sns.set(font_scale = 1.4)\n","sns.catplot(x = 'model_name', y = 'accuracy', hue = 'method', height = 7,\n","            data = all_df, kind = 'box', col = 'vectorizer', palette = 'rainbow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mq6rJToXaQcO"},"source":["### Naive Bayes and RandomForest do much worse than Logistic and SVC, and make the boxplots fairly hard to look at. Let's drop them for better visuals. "]},{"cell_type":"code","metadata":{"id":"ybtZhohQaQcR","executionInfo":{"status":"aborted","timestamp":1625760265297,"user_tz":-330,"elapsed":46,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["no_nb = all_df[all_df['model_name'] != 'NaiveBayes']\n","no_nb_rf = no_nb[no_nb['model_name'] != 'RandomForest']\n","sns.set(font_scale = 1.4)\n","sns.catplot(x = 'model_name', y = 'accuracy', hue = 'method', height = 7,\n","            data = no_nb_rf, kind = 'box', col = 'vectorizer', palette = 'rainbow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ix82xJx9aQcT"},"source":["### SVC does better when using tfidf, and Logistic Regression does better when using count. Stemming seems to do worse than lemmatization accuracy wise, although lemmatization has more outliers. The best results tend to come from using neither lemmatization nor stemming on the tweets. \n","### SVC using tfidf and Logistic with count have approximately the same median, but the SVC has less variance and a slightly more even distribution. \n","### It should be noted that the differences in accuracies between the best performing models are very small, and are probably due to the random splits more than methodology.  Bearing that in mind, I would select the LinearSVC using tfidf and no lemma/stem because it takes MUCH less time to run than the logistic regression, and based on these results, it has less variance. "]},{"cell_type":"code","metadata":{"id":"jJpHVPZPaQcX","executionInfo":{"status":"aborted","timestamp":1625760265298,"user_tz":-330,"elapsed":47,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["accuracies = all_df.groupby(['model_name', 'method', 'vectorizer']).accuracy.mean()\n","stdDev = all_df.groupby(['model_name', 'method', 'vectorizer']).accuracy.std()\n","metrics_df = pd.concat([accuracies, stdDev], axis = 1, ignore_index = True)\n","metrics_df.columns = ['mean_acc', 'mean_std']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JD4ahqVpaQcZ","executionInfo":{"status":"aborted","timestamp":1625760265298,"user_tz":-330,"elapsed":47,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["metrics_df.sort_values(by = ['mean_acc','method'], ascending = False).head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0LpIq8wxaQce"},"source":["## Again, this displays just how small the accuracy differences are between the best models. For the sake of efficiency, an SVC using tfidf vectors is recommended. Let's fit one and explore the results more thorouhgly. "]},{"cell_type":"code","metadata":{"id":"VpFTeoxYaQcf","executionInfo":{"status":"aborted","timestamp":1625760265299,"user_tz":-330,"elapsed":47,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Set X and y.\n","X = df['newTweet']\n","y = df['label']\n","\n","# Set vectorizer for feature extraction.\n","vectorizer = TfidfVectorizer()\n","\n","# Split data into training and test sets to fit the model.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","# Define model for predictions\n","model = Pipeline([('vectorizer',vectorizer),\n","                  ('clf', LinearSVC(max_iter = 5000))])\n","\n","model.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KZdT4tsbaQcg","executionInfo":{"status":"aborted","timestamp":1625760265299,"user_tz":-330,"elapsed":47,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["train_preds = model.predict(X_test)\n","\n","print('Accuracy:', accuracy_score(y_test, train_preds))\n","print('\\n')\n","print(classification_report(y_test, train_preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o3FBwcZhaQck"},"source":["### ~80% accuracy on the training data, not too bad. Precision and recall are significantly lower for neutral tweets than positive or negative, possibly due to the lower support, but it could also be that neutral tweets are harder to classify. This model appears to be slightly better at predicting positive tweets than negative tweets. \n","### Now, we'll see how the model performs on the test data."]},{"cell_type":"code","metadata":{"id":"uXZ7gY-aaQck","executionInfo":{"status":"aborted","timestamp":1625760265300,"user_tz":-330,"elapsed":48,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Set X and y.\n","X2 = test_df['newTweet']\n","y2 = test_df['label']\n","\n","\n","test_preds = model.predict(X2)\n","print('Accuracy:', accuracy_score(y2, test_preds))\n","print('\\n')\n","print(classification_report(y2, test_preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5fiOOsX5NbL","executionInfo":{"status":"aborted","timestamp":1625760265302,"user_tz":-330,"elapsed":50,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C12qgqlmLpyE"},"source":["### The model predicted negative for this tweet, while the true label is positive. Lower prices is indeed a positive thing for consumers. Perhaps the model took 'stuck', 'coronavirus', and 'covid' to be more negative than the other words in the tweet."]},{"cell_type":"markdown","metadata":{"id":"l65zJH6LLpyE"},"source":["### A possible update to improve accuracy of this model may involve handling the accented letters in a better way. \n","### However, if we want better accuracy, we should try BERT. We'll fit a BERT model and see how well it does."]},{"cell_type":"markdown","metadata":{"id":"qceJ4eeBLpyE"},"source":["# 5. BERT <a name=\"bert\"></a>"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"aBmHpmxWaQdN","executionInfo":{"status":"aborted","timestamp":1625760265303,"user_tz":-330,"elapsed":51,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["import torch\n","from tqdm.notebook import tqdm\n","\n","from transformers import BertTokenizer\n","\n","from torch.utils.data import TensorDataset\n","\n","import transformers\n","from transformers import BertForSequenceClassification\n","\n","#import numpy as np\n","#import pandas as pd\n","#import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-vgwf4ZWaQdP","executionInfo":{"status":"aborted","timestamp":1625760265304,"user_tz":-330,"elapsed":51,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Encode the classes for BERT. We'll keep using the 3 labels we made earlier.  \n","encoder = LabelEncoder()\n","df['encoded_sentiment'] = encoder.fit_transform(df['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFQdATPMaQdS","executionInfo":{"status":"aborted","timestamp":1625760265304,"user_tz":-330,"elapsed":51,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Set X and y.\n","X = df['newTweet']\n","y = df['encoded_sentiment']\n","\n","# Split data into training and test sets.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2yQejgiaQdV","executionInfo":{"status":"aborted","timestamp":1625760265304,"user_tz":-330,"elapsed":51,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YWyYJCIZaQdb","executionInfo":{"status":"aborted","timestamp":1625760265305,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Encoding the words in the training data into vectors.\n","encoded_data_train = tokenizer.batch_encode_plus(\n","    X_train, \n","    truncation = True,\n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    pad_to_max_length=True, \n","    max_length=50, \n","    return_tensors='pt'\n",")\n","\n","# Encoding the words in the test data into vectors.\n","encoded_data_test = tokenizer.batch_encode_plus(\n","    X_test, \n","    truncation = True,\n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    pad_to_max_length=True, \n","    max_length=50, \n","    return_tensors='pt'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpTiXmaUaQde","executionInfo":{"status":"aborted","timestamp":1625760265305,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Get inputs and attention masks from previously encoded data. \n","input_ids_train = encoded_data_train['input_ids']\n","attention_masks_train = encoded_data_train['attention_mask']\n","labels_train = torch.tensor(y_train.values)\n","\n","input_ids_test = encoded_data_test['input_ids']\n","attention_masks_test = encoded_data_test['attention_mask']\n","labels_test = torch.tensor(y_test.values)\n","\n","# Instantiate TensorDataset\n","dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLTjXXz2aQdg","executionInfo":{"status":"aborted","timestamp":1625760265305,"user_tz":-330,"elapsed":51,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Initialize the model. \n","model = transformers.BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=3,\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h8w2dJ8IaQdi","executionInfo":{"status":"aborted","timestamp":1625760265306,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# DataLoaders for running the model\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","dataloader_train = DataLoader(dataset_train, \n","                              sampler=RandomSampler(dataset_train), \n","                              batch_size=128)\n","\n","dataloader_test = DataLoader(dataset_test, \n","                                   sampler=SequentialSampler(dataset_test), \n","                                   batch_size=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hYoa1XM8aQdk","executionInfo":{"status":"aborted","timestamp":1625760265306,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["# Setting hyperparameters\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","optimizer = AdamW(model.parameters(),\n","                  lr=1e-5, \n","                  eps=1e-8)\n","                  \n","epochs = 10\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps=0,\n","                                            num_training_steps=len(dataloader_train)*epochs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iU6Z8A03aQdn","executionInfo":{"status":"aborted","timestamp":1625760265307,"user_tz":-330,"elapsed":53,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["from sklearn.metrics import f1_score\n","\n","def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjLpCzaPaQdo","executionInfo":{"status":"aborted","timestamp":1625760265307,"user_tz":-330,"elapsed":53,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["import random\n","\n","seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","device = torch.device('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ApM5Cs3Fqe7","executionInfo":{"status":"aborted","timestamp":1625760265307,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["model.to(device)\n","\n","for epoch in tqdm(range(1, epochs+1)):\n","    \n","    model.train()\n","    \n","    loss_train_total = 0\n","\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","    for batch in progress_bar:\n","\n","        model.zero_grad()\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0].to(device),\n","                  'attention_mask': batch[1].to(device),\n","                  'labels':         batch[2].to(device),\n","                 }       \n","\n","        outputs = model(**inputs)\n","        \n","        loss = outputs[0]\n","        loss_train_total += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","        \n","    tqdm.write(f'\\nEpoch {epoch}')\n","    \n","    loss_train_avg = loss_train_total/len(dataloader_train)            \n","    tqdm.write(f'Training loss: {loss_train_avg}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZfXiYRZFCB4","executionInfo":{"status":"aborted","timestamp":1625760265308,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["def evaluate(dataloader_test):\n","\n","    model.eval()\n","    \n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","    \n","    for batch in dataloader_test:\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","            \n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","    \n","    loss_val_avg = loss_val_total/len(dataloader_test) \n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","            \n","    return loss_val_avg, predictions, true_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6gcLaHjT4-X","executionInfo":{"status":"aborted","timestamp":1625760265308,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["val_loss, predictions, true_vals = evaluate(dataloader_test)\n","val_f1 = f1_score_func(predictions, true_vals)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbSo-pqxT5NW","executionInfo":{"status":"aborted","timestamp":1625760265308,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["print('Val Loss = ', val_loss)\n","print('Val F1 = ', val_f1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9siBCmgEjdq","executionInfo":{"status":"aborted","timestamp":1625760265308,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["encoded_classes = encoder.classes_\n","predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n","true_category = [encoded_classes[x] for x in true_vals]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1cJlYwA-TEse","executionInfo":{"status":"aborted","timestamp":1625760265309,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["x = 0\n","for i in range(len(true_category)):\n","    if true_category[i] == predicted_category[i]:\n","        x += 1\n","        \n","print('Accuracy Score = ', x / len(true_category))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8cnJf41TcFS","executionInfo":{"status":"aborted","timestamp":1625760265309,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["print(classification_report(true_category, predicted_category))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdLPZtecLpyh"},"source":["### 87% accuracy is about 7% better than what we get using an SVC for training. "]},{"cell_type":"markdown","metadata":{"id":"QJXD5vlPnr4F"},"source":["## Now, we'll use the test dataset to evaluate BERT."]},{"cell_type":"code","metadata":{"id":"gSi-Q2edlPWy","executionInfo":{"status":"aborted","timestamp":1625760265309,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["test_df['encoded_sentiment'] = encoder.fit_transform(test_df['label'])\n","\n","# Set X and y.\n","X = test_df['newTweet']\n","y = test_df['encoded_sentiment']\n","\n","encoded_data_test = tokenizer.batch_encode_plus(\n","    X, \n","    truncation = True,\n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    pad_to_max_length=True, \n","    max_length=50, \n","    return_tensors='pt'\n",")\n","\n","input_ids_test = encoded_data_test['input_ids']\n","attention_masks_test = encoded_data_test['attention_mask']\n","labels_test = torch.tensor(y.values)\n","\n","# Pytorch TensorDataset Instance\n","dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n","\n","dataloader_test = DataLoader(dataset_test, \n","                                   sampler=SequentialSampler(dataset_test), \n","                                   batch_size=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GLrztMPoFmO","executionInfo":{"status":"aborted","timestamp":1625760265310,"user_tz":-330,"elapsed":53,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["val_loss, predictions, true_vals = evaluate(dataloader_test)\n","val_f1 = f1_score_func(predictions, true_vals)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yca-88ISos7q","executionInfo":{"status":"aborted","timestamp":1625760265310,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":["encoded_classes = encoder.classes_\n","predicted_category = [encoded_classes[np.argmax(x)] for x in predictions]\n","true_category = [encoded_classes[x] for x in true_vals]\n","\n","x = 0\n","for i in range(len(true_category)):\n","    if true_category[i] == predicted_category[i]:\n","        x += 1\n","        \n","print('Accuracy Score = ', x / len(true_category))\n","print('\\n')\n","print(classification_report(true_category, predicted_category))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s30bQhY_Lpyl"},"source":["### On the actual test data, the model scores an 85%, which is ~6% better than the LinearSVC performed on this data. This BERT model could possibly squeeze out some more accuracy with additional hyperparameter tuning, as I did not play around with the learning rate. Also, you could try feeding the lemmatized or stemmed tweets into it, as for this run, I went with the cleaned tweets instead of the stemmed/lemmatized. "]},{"cell_type":"markdown","metadata":{"id":"1NumbhkhLpyn"},"source":["# 6. Conclusion <a name=\"conclusion\"></a>\n","\n","### Unsuprisingly, BERT performs better than an SVC or logistic regression. However, it was fairly shocking to see lemmatization and stemming perform a bit worse than just leaving the words alone. It was also a bit curious how the Random Forest Classifier lagged a bit behind the SVC and the logistic regression. We chose the SVC using TFIDF amongst the traditional classifiers because it was the most accurate and runs much faster than the logisitic regression on this data. But when it comes to raw accuracy, BERT is decidedly better than an SVC. \n","\n","### To further increase prediction accuracy, one should try tuning the hyperparameters of BERT, or testing other pretrained HuggingFace transformers on this dataset. "]},{"cell_type":"markdown","metadata":{"id":"iuHLV7_rLpyn"},"source":["### A big thank you to all these notebooks:\n","\n","https://www.kaggle.com/immvab/transformers-covid-19-tweets-sentiment-analysis/comments\n","\n","https://www.kaggle.com/arushi2/covid19-tweets-geo-and-sentiment-analysis#data\n","\n","https://www.kaggle.com/datatattle/battle-of-ml-classification-models\n","\n","https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\n","\n"]},{"cell_type":"code","metadata":{"id":"1nYKv4WzLpyo","executionInfo":{"status":"aborted","timestamp":1625760265310,"user_tz":-330,"elapsed":52,"user":{"displayName":"Girish L","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFYUUS_32MqAdpAZEVQJ6OCQY0GdRRslhgRs3N4A=s64","userId":"17486224615133974293"}}},"source":[""],"execution_count":null,"outputs":[]}]}